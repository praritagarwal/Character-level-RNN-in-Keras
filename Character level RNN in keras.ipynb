{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "anna = open('anna.txt', 'r')\n",
    "txt = anna.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### charcater tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " encode characters as integers. In Keras this can be done by the Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, the Tokenizer class tokenizes the words in the text rather than individual characters. This can be changed by setting char_level = True\n",
    "\n",
    "- The default tokenizer converts all alphabets to lower case, this can be changed by setting lower = False\n",
    "\n",
    "- The default tokeniser ignores all punctuations, tabs and line breaks etc. This can be changed by passing an explicit list of characters to the keyworded argument 'filters' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will keep all the default characters in the filter except line breaks, question marks, fullstops and exclamation marks\n",
    "fltr = '\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t' \n",
    "\n",
    "# create a tokenizer instance\n",
    "# we will also require the text to be Case sensitive\n",
    "tokenizer = Tokenizer(filters = fltr, lower = False, char_level = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the Tokenizer should be thought of analogously to data transformers in sklearn so we first fit then to our training text and the use the fitten tokenizer to transform any given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tokenizer\n",
    "# we can fit it on a list of multiple different texts\n",
    "tokenizer.fit_on_texts([txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37, 5, 15, 1, 4, 10, 2, 1, 19, 5, 14, 1, 11, 5, 8, 6, 18, 35]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chaecking the tokenizer indeed produces a sensible output\n",
    "sample = 'How are you doing?'\n",
    "\n",
    "tokenizer.texts_to_sequences([sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H o w   a r e   y o u   d o i n g ?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting a sequence back to text\n",
    "tokenizer.sequences_to_texts([[37, 5, 15, 1, 4, 10, 2, 1, 19, 5, 14, 1, 11, 5, 8, 6, 18, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct characters in the text: 83\n"
     ]
    }
   ],
   "source": [
    "# number of distinct characters in the text\n",
    "print('Number of distinct characters in the text: {}'.format(len(tokenizer.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters in the text: 1985223\n"
     ]
    }
   ],
   "source": [
    "# total number of characters in the text\n",
    "# the attribute .word_counts gives the number of times each character/word appears in the text\n",
    "# it returns an ordered dictionary with the characters as its keys and their count as the corresponding value\n",
    "character_count = tokenizer.word_counts\n",
    "\n",
    "# computations on elements of a list can be done efficiently through reduce \n",
    "# for e.g. see here : https://book.pythontips.com/en/latest/map_filter.html\n",
    "from functools import reduce\n",
    "\n",
    "total_chars = reduce(lambda x, y: x+y , character_count.values())\n",
    "print('total number of characters in the text: {}'.format(total_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53,  7,  4, ...,  9, 24, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the whole text to a sequence of integers using tokenizer\n",
    "encoded = np.array(tokenizer.texts_to_sequences([txt]))\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1985223)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sequences from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract split the text into sequences. One way to do this is to simply to appropriately reshape the array containing the text. Note that Aurelion Geron extracts the sequences by using window() method of tf.data.Dataset class. We will learn how to do this later, but for now let's just simply reshape the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected number of sequences in the text: 19655\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "num_seq = encoded.shape[1]//(seq_len+1) # we will include one extra character as the target character for the last time_step in the sequence\n",
    "print('expected number of sequences in the text: {}'.format(num_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19655, 101, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = encoded[:,:num_seq*(seq_len+1)].reshape(num_seq, seq_len+1,1)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the input and target sequences \n",
    "X = sequences[:,:seq_len,:]\n",
    "y = sequences[:, 1:seq_len+1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13],\n",
       "       [57],\n",
       "       [25],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [19],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57],\n",
       "       [25],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [19],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [18]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0, 90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "train_size = 0.75\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = train_size, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train: (14741, 100, 1)\n",
      "shape of y_train: (14741, 100, 1)\n",
      "shape of X_val: (4914, 100, 1)\n",
      "shape of y_val: (4914, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train: {}'.format(X_train.shape))\n",
    "print('shape of y_train: {}'.format(y_train.shape))\n",
    "print('shape of X_val: {}'.format(X_val.shape))\n",
    "print('shape of y_val: {}'.format(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the target sequences indeed represent the next character in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [17],\n",
       "       [14],\n",
       "       [16],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [14],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[200, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9],\n",
       "       [ 1],\n",
       "       [17],\n",
       "       [14],\n",
       "       [16],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [14],\n",
       "       [ 6],\n",
       "       [11]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[200, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [16],\n",
       "       [ 5],\n",
       "       [14],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 8],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[371, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [ 1],\n",
       "       [16],\n",
       "       [ 5],\n",
       "       [14],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [18]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[371, 90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encoding the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to given batch of sequence into a sequence of one-hot-encoded vectors of a given dimension\n",
    "# here we are assuming that the input array will be a 3d array of shape (batch_size, seq_length, 1)\n",
    "# we want the output to be a 3d array of shape (batch_size, seq_length, encoding_dim)\n",
    "\n",
    "def one_hot_encode(sequence, encoding_dim):\n",
    "    \n",
    "    # shape of the output array\n",
    "    out_shape = (sequence.size, encoding_dim)\n",
    "    \n",
    "    one_hot_arr = np.zeros(out_shape)\n",
    "    \n",
    "    one_hot_arr[np.arange(sequence.size), sequence.flatten()-1] = 1  # This is based on the fact that in numpyt, the operation\n",
    "                                                                   # X[[a1, a2, ..], [b1, b2, ...]] = y\n",
    "                                                                   # produces the same output as\n",
    "                                                                   # X[a1,b1] =y, X[a2,b2] = y, ....\n",
    "                \n",
    "    # reshape the one_hot_arr to have the shape shape as sequences \n",
    "    one_hot_arr = one_hot_arr.reshape(*sequence.shape[:-1], encoding_dim)\n",
    "    \n",
    "    return one_hot_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing one_hot_encode\n",
    "seq = np.array([[[1], [5], [6]],[[3],[8],[4]]])\n",
    "one_hot_encode(seq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of one_hot_encoded X_train: (14741, 100, 83)\n",
      "shape of one_hot_encoded y_train: (14741, 100, 83)\n",
      "shape of one_hot_encoded X_val: (4914, 100, 83)\n",
      "shape of one_hot_encoded y_val: (4914, 100, 83)\n"
     ]
    }
   ],
   "source": [
    "# now one_hot_encoding all the sequences \n",
    "num_chars = len(tokenizer.word_index) # number of distinct characters in the text\n",
    "\n",
    "\n",
    "X_train_one_hot = one_hot_encode(X_train, num_chars)\n",
    "print('shape of one_hot_encoded X_train: {}'.format(X_train_one_hot.shape))\n",
    "y_train_one_hot = one_hot_encode(y_train, num_chars)\n",
    "print('shape of one_hot_encoded y_train: {}'.format(y_train_one_hot.shape))\n",
    "X_val_one_hot = one_hot_encode(X_val, num_chars)\n",
    "print('shape of one_hot_encoded X_val: {}'.format(X_val_one_hot.shape))\n",
    "y_val_one_hot = one_hot_encode(y_val, num_chars)\n",
    "print('shape of one_hot_encoded y_val: {}'.format(y_val_one_hot.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training a GRU cell based RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "num_units = 20\n",
    "\n",
    "my_RNN = Sequential()\n",
    "my_RNN.add(GRU(units = num_units, input_shape = [None, num_chars], return_sequences = True, \n",
    "               dropout = dropout, recurrent_dropout = dropout))\n",
    "\n",
    "for layer in range(num_layers-1):\n",
    "    my_RNN.add(GRU(units = num_units, return_sequences = True, dropout = dropout, recurrent_dropout = dropout))\n",
    "\n",
    "my_RNN.add(Dense(units = num_chars, activation = 'softmax' )) # note that we can directly apply softmax activation to the Dense layer instead of addinga softmax layer on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy' # since we one_hot_encoded our target values, we will use categorical_crossentropy \n",
    "                                  # instead of sparse_categorical_crossentropy as loss\n",
    "my_RNN.compile(optimizer = optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping\n",
    "patience = 30\n",
    "min_delta = 0.1\n",
    "stopper = EarlyStopping(monitor = 'val_loss', patience = patience, min_delta = min_delta, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 14741 samples, validate on 4914 samples\n",
      "Epoch 1/100\n",
      "14741/14741 [==============================] - 25s 2ms/step - loss: 4.3892 - val_loss: 4.3432\n",
      "Epoch 2/100\n",
      "14741/14741 [==============================] - 21s 1ms/step - loss: 4.2613 - val_loss: 4.1090\n",
      "Epoch 3/100\n",
      "14741/14741 [==============================] - 22s 1ms/step - loss: 3.8734 - val_loss: 3.5528\n",
      "Epoch 4/100\n",
      "14741/14741 [==============================] - 22s 2ms/step - loss: 3.4089 - val_loss: 3.2702\n",
      "Epoch 5/100\n",
      "14741/14741 [==============================] - 24s 2ms/step - loss: 3.2253 - val_loss: 3.1857\n",
      "Epoch 6/100\n",
      "14741/14741 [==============================] - 23s 2ms/step - loss: 3.1732 - val_loss: 3.1600\n",
      "Epoch 7/100\n",
      "14741/14741 [==============================] - 22s 2ms/step - loss: 3.1536 - val_loss: 3.1469\n",
      "Epoch 8/100\n",
      "14741/14741 [==============================] - 22s 2ms/step - loss: 3.1433 - val_loss: 3.1395\n",
      "Epoch 9/100\n",
      "14741/14741 [==============================] - 22s 2ms/step - loss: 3.1369 - val_loss: 3.1346\n",
      "Epoch 10/100\n",
      "14741/14741 [==============================] - 25s 2ms/step - loss: 3.1326 - val_loss: 3.1310\n",
      "Epoch 11/100\n",
      "14741/14741 [==============================] - 23s 2ms/step - loss: 3.1294 - val_loss: 3.1283\n",
      "Epoch 12/100\n",
      "14741/14741 [==============================] - 23s 2ms/step - loss: 3.1268 - val_loss: 3.1261\n",
      "Epoch 13/100\n",
      "14741/14741 [==============================] - 26s 2ms/step - loss: 3.1248 - val_loss: 3.1243\n",
      "Epoch 14/100\n",
      "14741/14741 [==============================] - 27s 2ms/step - loss: 3.1231 - val_loss: 3.1228\n",
      "Epoch 15/100\n",
      "14741/14741 [==============================] - 28s 2ms/step - loss: 3.1217 - val_loss: 3.1215\n",
      "Epoch 16/100\n",
      "14741/14741 [==============================] - 25s 2ms/step - loss: 3.1204 - val_loss: 3.1204\n",
      "Epoch 17/100\n",
      "14741/14741 [==============================] - 26s 2ms/step - loss: 3.1193 - val_loss: 3.1194\n",
      "Epoch 18/100\n",
      "14741/14741 [==============================] - 27s 2ms/step - loss: 3.1184 - val_loss: 3.1185\n",
      "Epoch 19/100\n",
      "14741/14741 [==============================] - 25s 2ms/step - loss: 3.1175 - val_loss: 3.1177\n",
      "Epoch 20/100\n",
      "14741/14741 [==============================] - 25s 2ms/step - loss: 3.1168 - val_loss: 3.1170\n",
      "Epoch 21/100\n",
      "14741/14741 [==============================] - 24s 2ms/step - loss: 3.1161 - val_loss: 3.1163\n",
      "Epoch 22/100\n",
      "14741/14741 [==============================] - 36s 2ms/step - loss: 3.1155 - val_loss: 3.1157\n",
      "Epoch 23/100\n",
      "14741/14741 [==============================] - 32s 2ms/step - loss: 3.1149 - val_loss: 3.1152\n",
      "Epoch 24/100\n",
      "14741/14741 [==============================] - 31s 2ms/step - loss: 3.1144 - val_loss: 3.1147\n",
      "Epoch 25/100\n",
      "14741/14741 [==============================] - 29s 2ms/step - loss: 3.1139 - val_loss: 3.1142\n",
      "Epoch 26/100\n",
      "14741/14741 [==============================] - 28s 2ms/step - loss: 3.1134 - val_loss: 3.1138\n",
      "Epoch 27/100\n",
      "14741/14741 [==============================] - 27s 2ms/step - loss: 3.1130 - val_loss: 3.1134\n",
      "Epoch 28/100\n",
      "14741/14741 [==============================] - 32s 2ms/step - loss: 3.1126 - val_loss: 3.1130\n",
      "Epoch 29/100\n",
      "14741/14741 [==============================] - 28s 2ms/step - loss: 3.1123 - val_loss: 3.1127\n",
      "Epoch 30/100\n",
      "14741/14741 [==============================] - 27s 2ms/step - loss: 3.1119 - val_loss: 3.1124\n",
      "Epoch 31/100\n",
      "14741/14741 [==============================] - 24s 2ms/step - loss: 3.1116 - val_loss: 3.1120\n",
      "Epoch 32/100\n",
      "14741/14741 [==============================] - 27s 2ms/step - loss: 3.1113 - val_loss: 3.1118\n",
      "Epoch 33/100\n",
      "14741/14741 [==============================] - 27s 2ms/step - loss: 3.1110 - val_loss: 3.1115\n",
      "Epoch 34/100\n",
      "14741/14741 [==============================] - 26s 2ms/step - loss: 3.1108 - val_loss: 3.1112\n",
      "Epoch 35/100\n",
      "14741/14741 [==============================] - 24s 2ms/step - loss: 3.1105 - val_loss: 3.1110\n",
      "Epoch 36/100\n",
      "14741/14741 [==============================] - 25s 2ms/step - loss: 3.1102 - val_loss: 3.1108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1734c719128>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the RNN \n",
    "\n",
    "verbose = 1\n",
    "epochs = 100\n",
    "my_RNN.fit(X_train_one_hot, y_train_one_hot, batch_size = 1000, verbose = verbose, epochs = epochs , \n",
    "           callbacks = [stopper], validation_data = (X_val_one_hot, y_val_one_hot)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
