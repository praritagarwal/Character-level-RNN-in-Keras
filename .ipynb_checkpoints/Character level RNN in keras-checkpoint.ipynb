{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "anna = open('anna.txt', 'r')\n",
    "txt = anna.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### charcater tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " encode characters as integers. In Keras this can be done by the Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, the Tokenizer class tokenizes the words in the text rather than individual characters. This can be changed by setting char_level = True\n",
    "\n",
    "- The default tokenizer converts all alphabets to lower case, this can be changed by setting lower = False\n",
    "\n",
    "- The default tokeniser ignores all punctuations, tabs and line breaks etc. This can be changed by passing an explicit list of characters to the keyworded argument 'filters' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will keep all the default characters in the filter except line breaks, question marks, fullstops and exclamation marks\n",
    "fltr = '\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t' \n",
    "\n",
    "# create a tokenizer instance\n",
    "# we will also require the text to be Case sensitive\n",
    "tokenizer = Tokenizer(filters = fltr, lower = False, char_level = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the Tokenizer should be thought of analogously to data transformers in sklearn so we first fit then to our training text and the use the fitten tokenizer to transform any given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tokenizer\n",
    "# we can fit it on a list of multiple different texts\n",
    "tokenizer.fit_on_texts([txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37, 5, 15, 1, 4, 10, 2, 1, 19, 5, 14, 1, 11, 5, 8, 6, 18, 35]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chaecking the tokenizer indeed produces a sensible output\n",
    "sample = 'How are you doing?'\n",
    "\n",
    "tokenizer.texts_to_sequences([sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H o w   a r e   y o u   d o i n g ?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting a sequence back to text\n",
    "tokenizer.sequences_to_texts([[37, 5, 15, 1, 4, 10, 2, 1, 19, 5, 14, 1, 11, 5, 8, 6, 18, 35]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct characters in the text: 83\n"
     ]
    }
   ],
   "source": [
    "# number of distinct characters in the text\n",
    "print('Number of distinct characters in the text: {}'.format(len(tokenizer.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters in the text: 1985223\n"
     ]
    }
   ],
   "source": [
    "# total number of characters in the text\n",
    "# the attribute .word_counts gives the number of times each character/word appears in the text\n",
    "# it returns an ordered dictionary with the characters as its keys and their count as the corresponding value\n",
    "character_count = tokenizer.word_counts\n",
    "\n",
    "# computations on elements of a list can be done efficiently through reduce \n",
    "# for e.g. see here : https://book.pythontips.com/en/latest/map_filter.html\n",
    "from functools import reduce\n",
    "\n",
    "total_chars = reduce(lambda x, y: x+y , character_count.values())\n",
    "print('total number of characters in the text: {}'.format(total_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53,  7,  4, ...,  9, 24, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting the whole text to a sequence of integers using tokenizer\n",
    "encoded = np.array(tokenizer.texts_to_sequences([txt]))\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1985223)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sequences from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract split the text into sequences. One way to do this is to simply to appropriately reshape the array containing the text. Note that Aurelion Geron extracts the sequences by using window() method of tf.data.Dataset class. We will learn how to do this later, but for now let's just simply reshape the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected number of sequences in the text: 19655\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "num_seq = encoded.shape[1]//(seq_len+1) # we will include one extra character as the target character for the last time_step in the sequence\n",
    "print('expected number of sequences in the text: {}'.format(num_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19655, 101, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = encoded[:,:num_seq*(seq_len+1)].reshape(num_seq, seq_len+1,1)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the input and target sequences \n",
    "X = sequences[:,:seq_len,:]\n",
    "y = sequences[:, 1:seq_len+1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13],\n",
       "       [57],\n",
       "       [25],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [19],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57],\n",
       "       [25],\n",
       "       [ 2],\n",
       "       [10],\n",
       "       [19],\n",
       "       [ 3],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [18]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0, 90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "train_size = 0.75\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = train_size, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train: (14741, 100, 1)\n",
      "shape of y_train: (14741, 100, 1)\n",
      "shape of X_val: (4914, 100, 1)\n",
      "shape of y_val: (4914, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train: {}'.format(X_train.shape))\n",
    "print('shape of y_train: {}'.format(y_train.shape))\n",
    "print('shape of X_val: {}'.format(X_val.shape))\n",
    "print('shape of y_val: {}'.format(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the target sequences indeed represent the next character in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4],\n",
       "       [ 9],\n",
       "       [ 1],\n",
       "       [17],\n",
       "       [14],\n",
       "       [16],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [14],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[200, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9],\n",
       "       [ 1],\n",
       "       [17],\n",
       "       [14],\n",
       "       [16],\n",
       "       [ 7],\n",
       "       [ 1],\n",
       "       [14],\n",
       "       [ 6],\n",
       "       [11]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[200, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7],\n",
       "       [ 2],\n",
       "       [ 1],\n",
       "       [16],\n",
       "       [ 5],\n",
       "       [14],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 8],\n",
       "       [ 6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[371, 90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [ 1],\n",
       "       [16],\n",
       "       [ 5],\n",
       "       [14],\n",
       "       [ 6],\n",
       "       [ 3],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [18]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[371, 90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot-encoding the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to given batch of sequence into a sequence of one-hot-encoded vectors of a given dimension\n",
    "# here we are assuming that the input array will be a 3d array of shape (batch_size, seq_length, 1)\n",
    "# we want the output to be a 3d array of shape (batch_size, seq_length, encoding_dim)\n",
    "\n",
    "def one_hot_encode(sequence, encoding_dim):\n",
    "    \n",
    "    # shape of the output array\n",
    "    out_shape = (sequence.size, encoding_dim)\n",
    "    \n",
    "    one_hot_arr = np.zeros(out_shape)\n",
    "    \n",
    "    one_hot_arr[np.arange(sequence.size), sequence.flatten()-1] = 1  # This is based on the fact that in numpyt, the operation\n",
    "                                                                   # X[[a1, a2, ..], [b1, b2, ...]] = y\n",
    "                                                                   # produces the same output as\n",
    "                                                                   # X[a1,b1] =y, X[a2,b2] = y, ....\n",
    "                \n",
    "    # reshape the one_hot_arr to have the shape shape as sequences \n",
    "    one_hot_arr = one_hot_arr.reshape(*sequence.shape[:-1], encoding_dim)\n",
    "    \n",
    "    return one_hot_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing one_hot_encode\n",
    "seq = np.array([[[1], [5], [6]],[[3],[8],[4]]])\n",
    "one_hot_encode(seq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of one_hot_encoded X_train: (14741, 100, 83)\n",
      "shape of one_hot_encoded y_train: (14741, 100, 83)\n",
      "shape of one_hot_encoded X_val: (4914, 100, 83)\n",
      "shape of one_hot_encoded y_val: (4914, 100, 83)\n"
     ]
    }
   ],
   "source": [
    "# now one_hot_encoding all the sequences \n",
    "num_chars = len(tokenizer.word_index) # number of distinct characters in the text\n",
    "\n",
    "\n",
    "X_train_one_hot = one_hot_encode(X_train, num_chars)\n",
    "print('shape of one_hot_encoded X_train: {}'.format(X_train_one_hot.shape))\n",
    "y_train_one_hot = one_hot_encode(y_train, num_chars)\n",
    "print('shape of one_hot_encoded y_train: {}'.format(y_train_one_hot.shape))\n",
    "X_val_one_hot = one_hot_encode(X_val, num_chars)\n",
    "print('shape of one_hot_encoded X_val: {}'.format(X_val_one_hot.shape))\n",
    "y_val_one_hot = one_hot_encode(y_val, num_chars)\n",
    "print('shape of one_hot_encoded y_val: {}'.format(y_val_one_hot.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training a GRU cell based RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "num_units = 50\n",
    "\n",
    "my_RNN1 = Sequential()\n",
    "my_RNN1.add(GRU(units = num_units, input_shape = [None, num_chars], return_sequences = True, \n",
    "               dropout = dropout, recurrent_dropout = dropout))\n",
    "\n",
    "for layer in range(num_layers-1):\n",
    "    my_RNN1.add(GRU(units = num_units, return_sequences = True, dropout = dropout, recurrent_dropout = dropout))\n",
    "\n",
    "my_RNN1.add(Dense(units = num_chars, activation = 'softmax' )) # note that we can directly apply softmax activation to the Dense layer instead of addinga softmax layer on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy' # since we one_hot_encoded our target values, we will use categorical_crossentropy \n",
    "                                  # instead of sparse_categorical_crossentropy as loss\n",
    "my_RNN1.compile(optimizer = optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping\n",
    "patience = 30\n",
    "min_delta = 0.1\n",
    "stopper = EarlyStopping(monitor = 'val_loss', patience = patience, min_delta = min_delta, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14741 samples, validate on 4914 samples\n",
      "Epoch 1/300\n",
      "14741/14741 [==============================] - 47s 3ms/step - loss: 2.0531 - val_loss: 1.8704\n",
      "Epoch 2/300\n",
      "14741/14741 [==============================] - 52s 3ms/step - loss: 2.0502 - val_loss: 1.8685\n",
      "Epoch 3/300\n",
      "14741/14741 [==============================] - 49s 3ms/step - loss: 2.0461 - val_loss: 1.8655\n",
      "Epoch 4/300\n",
      "14741/14741 [==============================] - 48s 3ms/step - loss: 2.0452 - val_loss: 1.8629\n",
      "Epoch 5/300\n",
      "14741/14741 [==============================] - 45s 3ms/step - loss: 2.0437 - val_loss: 1.8609\n",
      "Epoch 6/300\n",
      "14741/14741 [==============================] - 51s 3ms/step - loss: 2.0396 - val_loss: 1.8578\n",
      "Epoch 7/300\n",
      "14741/14741 [==============================] - 53s 4ms/step - loss: 2.0404 - val_loss: 1.8559\n",
      "Epoch 8/300\n",
      "14741/14741 [==============================] - 61s 4ms/step - loss: 2.0361 - val_loss: 1.8538\n",
      "Epoch 9/300\n",
      "14741/14741 [==============================] - 64s 4ms/step - loss: 2.0332 - val_loss: 1.8514\n",
      "Epoch 10/300\n",
      "14741/14741 [==============================] - 52s 4ms/step - loss: 2.0327 - val_loss: 1.8491\n",
      "Epoch 11/300\n",
      "14741/14741 [==============================] - 53s 4ms/step - loss: 2.0299 - val_loss: 1.8465\n",
      "Epoch 12/300\n",
      "14741/14741 [==============================] - 57s 4ms/step - loss: 2.0278 - val_loss: 1.8440\n",
      "Epoch 13/300\n",
      "14741/14741 [==============================] - 55s 4ms/step - loss: 2.0264 - val_loss: 1.8417\n",
      "Epoch 14/300\n",
      "14741/14741 [==============================] - 51s 3ms/step - loss: 2.0248 - val_loss: 1.8401\n",
      "Epoch 15/300\n",
      "14741/14741 [==============================] - 56s 4ms/step - loss: 2.0229 - val_loss: 1.8379\n",
      "Epoch 16/300\n",
      "14741/14741 [==============================] - 49s 3ms/step - loss: 2.0225 - val_loss: 1.8363\n",
      "Epoch 17/300\n",
      "14741/14741 [==============================] - 49s 3ms/step - loss: 2.0184 - val_loss: 1.8347\n",
      "Epoch 18/300\n",
      "14741/14741 [==============================] - 52s 4ms/step - loss: 2.0182 - val_loss: 1.8321\n",
      "Epoch 19/300\n",
      "14741/14741 [==============================] - 46s 3ms/step - loss: 2.0155 - val_loss: 1.8303\n",
      "Epoch 20/300\n",
      "14741/14741 [==============================] - 49s 3ms/step - loss: 2.0138 - val_loss: 1.8284\n",
      "Epoch 21/300\n",
      "14741/14741 [==============================] - 47s 3ms/step - loss: 2.0130 - val_loss: 1.8265\n",
      "Epoch 22/300\n",
      "14741/14741 [==============================] - 48s 3ms/step - loss: 2.0110 - val_loss: 1.8245\n",
      "Epoch 23/300\n",
      "14741/14741 [==============================] - 47s 3ms/step - loss: 2.0100 - val_loss: 1.8228\n",
      "Epoch 24/300\n",
      "14741/14741 [==============================] - 47s 3ms/step - loss: 2.0069 - val_loss: 1.8211\n",
      "Epoch 25/300\n",
      "14741/14741 [==============================] - 56s 4ms/step - loss: 2.0064 - val_loss: 1.8191\n",
      "Epoch 26/300\n",
      "14741/14741 [==============================] - 52s 3ms/step - loss: 2.0039 - val_loss: 1.8177\n",
      "Epoch 27/300\n",
      "14741/14741 [==============================] - 52s 4ms/step - loss: 2.0021 - val_loss: 1.8158\n",
      "Epoch 28/300\n",
      "14741/14741 [==============================] - 51s 3ms/step - loss: 2.0020 - val_loss: 1.8144\n",
      "Epoch 29/300\n",
      "14741/14741 [==============================] - 49s 3ms/step - loss: 1.9991 - val_loss: 1.8124\n",
      "Epoch 30/300\n",
      "14741/14741 [==============================] - 51s 3ms/step - loss: 1.9991 - val_loss: 1.8104\n",
      "Epoch 31/300\n",
      "14741/14741 [==============================] - 47s 3ms/step - loss: 1.9971 - val_loss: 1.8094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1adcf7b7320>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the RNN \n",
    "\n",
    "verbose = 1\n",
    "epochs = 300\n",
    "my_RNN1.fit(X_train_one_hot, y_train_one_hot, batch_size = 1000, verbose = verbose, epochs = epochs , \n",
    "           callbacks = [stopper], validation_data = (X_val_one_hot, y_val_one_hot)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a new text using the trained RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prime the model with some initial starting string\n",
    "starting_string = 'Where are yo'\n",
    "# tokenize the starting string\n",
    "starting_string_tokens = np.array(tokenizer.texts_to_sequences(starting_string))\n",
    "# one_hot_encode the starting string\n",
    "start_one_hot = np.array([one_hot_encode(starting_string_tokens, num_chars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1,  0,  0,  0,  2,  5, 10,  0,  2,  4, 13]], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if the model is able to predict the character 'u' to complere the starting string?\n",
    "output_txt = my_RNN1.predict_classes(start_one_hot)\n",
    "output_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect that the predicted character at the n-th time step is the n+1-th character in the starting string\n",
    "# the last predicted character is expected to be 'u' to complete the word 'yo' to 'you' in th starting string\n",
    "tokenizer.sequences_to_texts(output_txt+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(output_txt+1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 83)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_RNN1.predict(start_one_hot).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(tokenized_text, model, topk = 5):\n",
    "    one_hot_text = np.array([one_hot_encode(tokenized_text, num_chars)])\n",
    "    pred_char_prob = model.predict(one_hot_text)[0,-1,:] # choose the predicted probabilities for the last time_step in the sequence\n",
    "    top_prob_arg = np.argsort(pred_char_prob)[-topk:] # np.argsort sorts the values in increasing order and returns their original arguments\n",
    "                                                      # to choose the last n elements of a numnpy array x, just call x[-n:]\n",
    "                                                      # the negative index makes it count from the end\n",
    "                                                      # https://stackoverflow.com/questions/646644/how-to-get-last-items-of-a-list-in-python\n",
    "    char = np.random.choice(top_prob_arg+1, p = pred_char_prob[top_prob_arg]/np.sum(pred_char_prob[top_prob_arg]) )\n",
    "    \n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(starting_text, model, desired_length = 100):\n",
    "    tokenize_starting_text = tokenizer.texts_to_sequences(starting_text)\n",
    "    generated_text = tokenize_starting_text\n",
    "    \n",
    "    for itr in range(desired_length):\n",
    "        nxt = next_char(np.array(generated_text), model)\n",
    "        generated_text.append([nxt])\n",
    "    \n",
    "    output = tokenizer.sequences_to_texts(generated_text)\n",
    "    \n",
    "    return ''.join(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where are you, and a mersing a stang, and the whette that thas her\\nwith and this said the pone the prentious wifh that to would somat was wherien and that they as the come on the homess, the somen of he was\\nwen w'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(starting_string, my_RNN1, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have learnt some words but doesn't look as impressive as the LSTM based sequence generator based on PyTorch implementation in the Udacity execise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cell based RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "num_layers = 2\n",
    "num_units = 50\n",
    "my_RNN2 = Sequential()\n",
    "my_RNN2.add(LSTM(units = num_units, input_shape = [None, num_chars], return_sequences = True, \n",
    "                 dropout = dropout, recurrent_dropout = dropout) )\n",
    "for itr in range(num_layers-1):\n",
    "    my_RNN2.add(LSTM(units = num_units, return_sequences = True, dropout = dropout, recurrent_dropout = dropout ))\n",
    "    \n",
    "my_RNN2.add(Dense(units = num_chars, activation = 'softmax'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "my_RNN2.compile( optimizer = optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "patience = 30\n",
    "min_delta = 0.1\n",
    "stopper = EarlyStopping( monitor = 'val_loss', min_delta = min_delta, patience = patience, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\agarw\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 14741 samples, validate on 4914 samples\n",
      "Epoch 1/100\n",
      "14741/14741 [==============================] - 125s 8ms/step - loss: 4.3251 - val_loss: 4.0142\n",
      "Epoch 2/100\n",
      "14741/14741 [==============================] - 116s 8ms/step - loss: 3.6537 - val_loss: 3.3538\n",
      "Epoch 3/100\n",
      "14741/14741 [==============================] - 113s 8ms/step - loss: 3.2616 - val_loss: 3.1944\n",
      "Epoch 4/100\n",
      "14741/14741 [==============================] - 119s 8ms/step - loss: 3.1754 - val_loss: 3.1599\n",
      "Epoch 5/100\n",
      "14741/14741 [==============================] - 123s 8ms/step - loss: 3.1536 - val_loss: 3.1483\n",
      "Epoch 6/100\n",
      "14741/14741 [==============================] - 124s 8ms/step - loss: 3.1452 - val_loss: 3.1422\n",
      "Epoch 7/100\n",
      "14741/14741 [==============================] - 122s 8ms/step - loss: 3.1400 - val_loss: 3.1380\n",
      "Epoch 8/100\n",
      "14741/14741 [==============================] - 118s 8ms/step - loss: 3.1362 - val_loss: 3.1344\n",
      "Epoch 9/100\n",
      "14741/14741 [==============================] - 124s 8ms/step - loss: 3.1328 - val_loss: 3.1313\n",
      "Epoch 10/100\n",
      "14741/14741 [==============================] - 121s 8ms/step - loss: 3.1297 - val_loss: 3.1283\n",
      "Epoch 11/100\n",
      "14741/14741 [==============================] - 124s 8ms/step - loss: 3.1267 - val_loss: 3.1250\n",
      "Epoch 12/100\n",
      "14741/14741 [==============================] - 125s 9ms/step - loss: 3.1232 - val_loss: 3.1199\n",
      "Epoch 13/100\n",
      "14741/14741 [==============================] - 125s 8ms/step - loss: 3.1180 - val_loss: 3.1125\n",
      "Epoch 14/100\n",
      "14741/14741 [==============================] - 128s 9ms/step - loss: 3.1107 - val_loss: 3.1031\n",
      "Epoch 15/100\n",
      "14741/14741 [==============================] - 141s 10ms/step - loss: 3.1009 - val_loss: 3.0910\n",
      "Epoch 16/100\n",
      "14741/14741 [==============================] - 148s 10ms/step - loss: 3.0890 - val_loss: 3.0766\n",
      "Epoch 17/100\n",
      "14741/14741 [==============================] - 145s 10ms/step - loss: 3.0746 - val_loss: 3.0598\n",
      "Epoch 18/100\n",
      "14741/14741 [==============================] - 157s 11ms/step - loss: 3.0590 - val_loss: 3.0422\n",
      "Epoch 19/100\n",
      "14741/14741 [==============================] - 145s 10ms/step - loss: 3.0428 - val_loss: 3.0241\n",
      "Epoch 20/100\n",
      "14741/14741 [==============================] - 174s 12ms/step - loss: 3.0272 - val_loss: 3.0071\n",
      "Epoch 21/100\n",
      "14741/14741 [==============================] - 163s 11ms/step - loss: 3.0120 - val_loss: 2.9911\n",
      "Epoch 22/100\n",
      "14741/14741 [==============================] - 134s 9ms/step - loss: 2.9983 - val_loss: 2.9764\n",
      "Epoch 23/100\n",
      "14741/14741 [==============================] - 138s 9ms/step - loss: 2.9859 - val_loss: 2.9628\n",
      "Epoch 24/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.9739 - val_loss: 2.9500\n",
      "Epoch 25/100\n",
      "14741/14741 [==============================] - 142s 10ms/step - loss: 2.9630 - val_loss: 2.9377\n",
      "Epoch 26/100\n",
      "14741/14741 [==============================] - 129s 9ms/step - loss: 2.9520 - val_loss: 2.9258\n",
      "Epoch 27/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.9421 - val_loss: 2.9138\n",
      "Epoch 28/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.9316 - val_loss: 2.9012\n",
      "Epoch 29/100\n",
      "14741/14741 [==============================] - 125s 8ms/step - loss: 2.9202 - val_loss: 2.8881\n",
      "Epoch 30/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.9090 - val_loss: 2.8744\n",
      "Epoch 31/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.8968 - val_loss: 2.8597\n",
      "Epoch 32/100\n",
      "14741/14741 [==============================] - 137s 9ms/step - loss: 2.8844 - val_loss: 2.8439\n",
      "Epoch 33/100\n",
      "14741/14741 [==============================] - 130s 9ms/step - loss: 2.8698 - val_loss: 2.8268\n",
      "Epoch 34/100\n",
      "14741/14741 [==============================] - 127s 9ms/step - loss: 2.8551 - val_loss: 2.8077\n",
      "Epoch 35/100\n",
      "14741/14741 [==============================] - 131s 9ms/step - loss: 2.8388 - val_loss: 2.7872\n",
      "Epoch 36/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.8213 - val_loss: 2.7657\n",
      "Epoch 37/100\n",
      "14741/14741 [==============================] - 128s 9ms/step - loss: 2.8039 - val_loss: 2.7441\n",
      "Epoch 38/100\n",
      "14741/14741 [==============================] - 130s 9ms/step - loss: 2.7884 - val_loss: 2.7234\n",
      "Epoch 39/100\n",
      "14741/14741 [==============================] - 127s 9ms/step - loss: 2.7715 - val_loss: 2.7040\n",
      "Epoch 40/100\n",
      "14741/14741 [==============================] - 130s 9ms/step - loss: 2.7579 - val_loss: 2.6863\n",
      "Epoch 41/100\n",
      "14741/14741 [==============================] - 129s 9ms/step - loss: 2.7439 - val_loss: 2.6698\n",
      "Epoch 42/100\n",
      "14741/14741 [==============================] - 131s 9ms/step - loss: 2.7313 - val_loss: 2.6543\n",
      "Epoch 43/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.7201 - val_loss: 2.6400\n",
      "Epoch 44/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.7089 - val_loss: 2.6267\n",
      "Epoch 45/100\n",
      "14741/14741 [==============================] - 131s 9ms/step - loss: 2.6986 - val_loss: 2.6137\n",
      "Epoch 46/100\n",
      "14741/14741 [==============================] - 131s 9ms/step - loss: 2.6886 - val_loss: 2.6018\n",
      "Epoch 47/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.6795 - val_loss: 2.5906\n",
      "Epoch 48/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.6712 - val_loss: 2.5795\n",
      "Epoch 49/100\n",
      "14741/14741 [==============================] - 129s 9ms/step - loss: 2.6629 - val_loss: 2.5698\n",
      "Epoch 50/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.6563 - val_loss: 2.5604\n",
      "Epoch 51/100\n",
      "14741/14741 [==============================] - 131s 9ms/step - loss: 2.6489 - val_loss: 2.5513\n",
      "Epoch 52/100\n",
      "14741/14741 [==============================] - 134s 9ms/step - loss: 2.6416 - val_loss: 2.5428\n",
      "Epoch 53/100\n",
      "14741/14741 [==============================] - 134s 9ms/step - loss: 2.6349 - val_loss: 2.5346\n",
      "Epoch 54/100\n",
      "14741/14741 [==============================] - 128s 9ms/step - loss: 2.6272 - val_loss: 2.5264\n",
      "Epoch 55/100\n",
      "14741/14741 [==============================] - 135s 9ms/step - loss: 2.6202 - val_loss: 2.5183\n",
      "Epoch 56/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.6130 - val_loss: 2.5100\n",
      "Epoch 57/100\n",
      "14741/14741 [==============================] - 139s 9ms/step - loss: 2.6061 - val_loss: 2.5023\n",
      "Epoch 58/100\n",
      "14741/14741 [==============================] - 141s 10ms/step - loss: 2.5980 - val_loss: 2.4942\n",
      "Epoch 59/100\n",
      "14741/14741 [==============================] - 134s 9ms/step - loss: 2.5898 - val_loss: 2.4863\n",
      "Epoch 60/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.5840 - val_loss: 2.4784\n",
      "Epoch 61/100\n",
      "14741/14741 [==============================] - 131s 9ms/step - loss: 2.5746 - val_loss: 2.4702\n",
      "Epoch 62/100\n",
      "14741/14741 [==============================] - 134s 9ms/step - loss: 2.5669 - val_loss: 2.4622\n",
      "Epoch 63/100\n",
      "14741/14741 [==============================] - 138s 9ms/step - loss: 2.5576 - val_loss: 2.4541\n",
      "Epoch 64/100\n",
      "14741/14741 [==============================] - 135s 9ms/step - loss: 2.5493 - val_loss: 2.4458\n",
      "Epoch 65/100\n",
      "14741/14741 [==============================] - 136s 9ms/step - loss: 2.5389 - val_loss: 2.4368\n",
      "Epoch 66/100\n",
      "14741/14741 [==============================] - 140s 9ms/step - loss: 2.5288 - val_loss: 2.4282\n",
      "Epoch 67/100\n",
      "14741/14741 [==============================] - 135s 9ms/step - loss: 2.5193 - val_loss: 2.4198\n",
      "Epoch 68/100\n",
      "14741/14741 [==============================] - 135s 9ms/step - loss: 2.5107 - val_loss: 2.4116\n",
      "Epoch 69/100\n",
      "14741/14741 [==============================] - 136s 9ms/step - loss: 2.5013 - val_loss: 2.4036\n",
      "Epoch 70/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.4927 - val_loss: 2.3949\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14741/14741 [==============================] - 156s 11ms/step - loss: 2.4846 - val_loss: 2.3872\n",
      "Epoch 72/100\n",
      "14741/14741 [==============================] - 188s 13ms/step - loss: 2.4762 - val_loss: 2.3789\n",
      "Epoch 73/100\n",
      "14741/14741 [==============================] - 179s 12ms/step - loss: 2.4688 - val_loss: 2.3713\n",
      "Epoch 74/100\n",
      "14741/14741 [==============================] - 159s 11ms/step - loss: 2.4607 - val_loss: 2.3627\n",
      "Epoch 75/100\n",
      "14741/14741 [==============================] - 150s 10ms/step - loss: 2.4532 - val_loss: 2.3555\n",
      "Epoch 76/100\n",
      "14741/14741 [==============================] - 142s 10ms/step - loss: 2.4469 - val_loss: 2.3481\n",
      "Epoch 77/100\n",
      "14741/14741 [==============================] - 137s 9ms/step - loss: 2.4392 - val_loss: 2.3401\n",
      "Epoch 78/100\n",
      "14741/14741 [==============================] - 142s 10ms/step - loss: 2.4334 - val_loss: 2.3330\n",
      "Epoch 79/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.4261 - val_loss: 2.3257\n",
      "Epoch 80/100\n",
      "14741/14741 [==============================] - 139s 9ms/step - loss: 2.4184 - val_loss: 2.3190\n",
      "Epoch 81/100\n",
      "14741/14741 [==============================] - 134s 9ms/step - loss: 2.4135 - val_loss: 2.3124\n",
      "Epoch 82/100\n",
      "14741/14741 [==============================] - 138s 9ms/step - loss: 2.4063 - val_loss: 2.3055\n",
      "Epoch 83/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.4002 - val_loss: 2.2982\n",
      "Epoch 84/100\n",
      "14741/14741 [==============================] - 139s 9ms/step - loss: 2.3945 - val_loss: 2.2920\n",
      "Epoch 85/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.3882 - val_loss: 2.2856\n",
      "Epoch 86/100\n",
      "14741/14741 [==============================] - 131s 9ms/step - loss: 2.3829 - val_loss: 2.2794\n",
      "Epoch 87/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.3764 - val_loss: 2.2728\n",
      "Epoch 88/100\n",
      "14741/14741 [==============================] - 138s 9ms/step - loss: 2.3714 - val_loss: 2.2667\n",
      "Epoch 89/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.3643 - val_loss: 2.2597\n",
      "Epoch 90/100\n",
      "14741/14741 [==============================] - 135s 9ms/step - loss: 2.3590 - val_loss: 2.2539\n",
      "Epoch 91/100\n",
      "14741/14741 [==============================] - 136s 9ms/step - loss: 2.3543 - val_loss: 2.2476\n",
      "Epoch 92/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.3482 - val_loss: 2.2415\n",
      "Epoch 93/100\n",
      "14741/14741 [==============================] - 132s 9ms/step - loss: 2.3427 - val_loss: 2.2354\n",
      "Epoch 94/100\n",
      "14741/14741 [==============================] - 133s 9ms/step - loss: 2.3376 - val_loss: 2.2292\n",
      "Epoch 95/100\n",
      "14741/14741 [==============================] - 134s 9ms/step - loss: 2.3323 - val_loss: 2.2242\n",
      "Epoch 96/100\n",
      "14741/14741 [==============================] - 130s 9ms/step - loss: 2.3260 - val_loss: 2.2172\n",
      "Epoch 97/100\n",
      "14741/14741 [==============================] - 130s 9ms/step - loss: 2.3221 - val_loss: 2.2115\n",
      "Epoch 98/100\n",
      "14741/14741 [==============================] - 130s 9ms/step - loss: 2.3176 - val_loss: 2.2060\n",
      "Epoch 99/100\n",
      "14741/14741 [==============================] - 136s 9ms/step - loss: 2.3121 - val_loss: 2.2009\n",
      "Epoch 100/100\n",
      "14741/14741 [==============================] - 135s 9ms/step - loss: 2.3069 - val_loss: 2.1951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e9034e4c18>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 100\n",
    "verbose = 1\n",
    "batch_size = 1000\n",
    "my_RNN2.fit(X_train_one_hot, y_train_one_hot, verbose = verbose, epochs = epochs, batch_size = 1000, \n",
    "            callbacks = [stopper], validation_data = (X_val_one_hot, y_val_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_string = 'Where are yo'\n",
    "# tokenize starting string \n",
    "starting_string_tokens = np.array(tokenizer.texts_to_sequences(starting_string))\n",
    "# one hot encode\n",
    "starting_string_one_hot =np.array([ one_hot_encode(starting_string_tokens, num_chars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1,  9,  0,  0,  2,  5, 10,  0,  2,  4, 13]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = my_RNN2.predict_classes(starting_string_one_hot)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h e r     t n d   t o u']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We expect that the predicted character at the n-th time step is the n+1-th character in the starting string\n",
    "# the last predicted character is expected to be 'u' to complete the word 'yo' to 'you' in th starting string\n",
    "predicted_chars = tokenizer.sequences_to_texts(output+1)\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_char = tokenizer.sequences_to_texts(output+1)[0][-1]\n",
    "last_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where are yo wely,\\n\\nThe thing she sent and wand the sing souserer, asererint ald sit and his to thand the whas he tat toe wonese ally and seined wisk oule sall thane as ard wing wan sore sor of has sout wand woth'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now generating new text\n",
    "# we will simply use the generate_text function we defined in the previous section\n",
    "generate_text(starting_string, my_RNN2, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
